# policy evalution, policy improvement

import copy
import time
import numpy as np
from r01_env_agent import Environment, Agent
from r02_show import show_policy, show_v_table


# ì •ì±… í‰ê°€ í•¨ìˆ˜
def policy_evaluation(env, agent, v_table, policy):
    while True:
        delta = 0
        temp_v = copy.deepcopy(v_table)

        for i in range(env.reward.shape[0]):
            for j in range(env.reward.shape[1]):
                # ì—ì´ì „íŠ¸ë¥¼ ì§€ì •ëœ ì¢Œí‘œì— ìœ„ì¹˜ì‹œí‚¨ í›„ ê°€ì¹˜í•¨ìˆ˜ ê³„ì‚°
                agent.set_pos([i, j])

                # í˜„ì¬ ì •ì±…ì˜ í–‰ë™ ì„ íƒ
                action = policy[i, j]
                observation, reward, done = env.move(agent, action)
                v_table[i, j] = (
                    reward
                    + gamma
                    * v_table[
                        observation[0],
                        observation[1],
                    ]
                )

        # ê³„ì‚° ì „í›„ì˜ ì°¨ì´ ê³„ì‚°
        delta = np.max([delta, np.max(np.abs(temp_v - v_table))])

        # delta < thetaì¼ ë•Œê¹Œì§€ ë°˜ë³µ
        if delta < 0.000001:
            break

    return v_table, delta


# ì •ì±… ê°œì„  í•¨ìˆ˜
def policy_improvement(env, agent, v_table, policy):
    policy_stable = True

    for i in range(env.reward.shape[0]):
        for j in range(env.reward.shape[1]):
            old_action = policy[i, j]

            # ê°€ëŠ¥í•œ í–‰ë™ ì¤‘ ìµœëŒ“ê°’ì„ ê°€ì§€ëŠ” í–‰ë™ ì„ íƒ
            temp_action = 0
            temp_value = -1e10
            for action in range(len(agent.action)):
                agent.set_pos([i, j])
                observation, reward, done = env.move(agent, action)
                # 'ê¸°ì¡´ ìƒíƒœ ê°€ì¹˜ * ê°ê°€ìœ¨'ì— ë³´ìƒì„ ë”í•œ ê°’ ì¤‘ ìµœëŒ“ê°’ì„ ê°€ì ¸ì˜¤ëŠ” ë°©í–¥(action) ë°˜í™˜
                if (
                    temp_value
                    < reward + gamma * v_table[observation[0], observation[1]]
                ):
                    temp_action = action
                    temp_value = (
                        reward + gamma * v_table[observation[0], observation[1]]
                    )

            # ë§Œì•½ old_actionì´ í˜„ì¬ ì •ì±…ê³¼ ë‹¤ë¥´ë‹¤ë©´ policy_stabel <- False
            # old_actionê³¼ ìƒˆë¡œìš´ actionì´ ë‹¤ë¥¸ì§€ ì²´í¬
            if old_action != temp_action:
                policy_stable = False
            policy[i, j] = temp_action

    return policy, policy_stable


# ì •ì±… ë°˜ë³µ
# í™˜ê²½ê³¼ ì—ì´ì „íŠ¸ ì´ˆê¸° ì„¤ì •
np.random.seed(0)
env = Environment()
agent = Agent()
gamma = 0.9
k = 1

# 1. ì´ˆê¸°í™”
# ëª¨ë“  ğ‘ âˆˆğ‘†ì— ëŒ€í•´ ğ‘‰(ğ‘ )âˆˆğ‘…ê³¼ Ï€(ğ‘ )âˆˆğ´(ğ‘ )ë¥¼ ì„ì˜ë¡œ ì„¤ì •
# v_table(3x3) <- 0-1 ì‚¬ì´ ë‚œìˆ˜
# policy(3x3) <- 0-3 ì‚¬ì´ ì •ìˆ˜ : ì²˜ìŒì—ëŠ” ë°©í–¥ì„ ë¬´ì‘ìœ„ë¡œ ì„ íƒ
v_table = np.random.rand(env.reward.shape[0], env.reward.shape[1])
policy = np.random.randint(
    0, 4, (env.reward.shape[0], env.reward.shape[1])
)  # low, high, size(output shape)

print("Initial random V(S)")
show_v_table(np.round(v_table, 2), env)
print()
print("Initial random Policy Ï€0(S)")
show_policy(policy, env)
print("start policy iteration")

# ì‹œì‘ ì‹œê°„ì„ ë³€ìˆ˜ì— ì €ì¥
start_time = time.time()

max_iter_number = 20000
for iter_number in range(max_iter_number):
    # 2. ì •ì±… í‰ê°€
    v_table, delta = policy_evaluation(env, agent, v_table, policy)

    # ì •ì±… í‰ê°€ í›„ ê²°ê³¼ í‘œì‹œ(ìƒíƒœ ê°€ì¹˜)
    print("")
    print("VÏ€{0:}(S) delta = {1:.10f}".format(iter_number, delta))
    show_v_table(np.round(v_table, 2), env)
    print()

    # 3. ì •ì±… ê°œì„ 
    policy, policy_stable = policy_improvement(env, agent, v_table, policy)

    # ì •ì±… ê°œì„  í›„ ì›€ì§ì¼ ë°©í–¥ í‘œì‹œ
    print("policy Ï€{}(S)".format(iter_number + 1))
    show_policy(policy, env)

    # í•˜ë‚˜ë¼ë„ old_actionê³¼ new_actionì´ ë‹¤ë¥´ë©´ '2.ì •ì±… í‰ê°€' ë°˜ë³µ
    if policy_stable:
        break

    k += 1

print("total_time = {}".format(time.time() - start_time))
