# Double Q-learning

import numpy as np
from tqdm import tqdm
from r01_env_agent import Environment, Agent
from r02_show import show_q_table, show_policy


# ğœ–âˆ’ğ‘”ğ‘Ÿğ‘’ğ‘’ğ‘‘ğ‘¦ í•¨ìˆ˜ ì‘ì„±
def e_greedy(Q_table, agent, epsilon):
    pos = agent.get_pos()
    greedy_action = np.argmax(Q_table[pos[0], pos[1]])
    pr = np.zeros(4)
    for i in range(len(agent.action)):
        if i == greedy_action:
            pr[i] = 1 - epsilon + epsilon / len(agent.action)
        else:
            pr[i] = epsilon / len(agent.action)
    # ì—…ë°ì´íŠ¸ ëœ pr í™•ë¥ ì„ ë°˜ì˜í•´ì„œ ë°©í–¥ì„ ëœë¤ ì¶”ì¶œí•˜ì—¬ ë°˜í™˜
    return np.random.choice(range(0, len(agent.action)), p=pr)


# ğ‘”ğ‘Ÿğ‘’ğ‘’ğ‘‘ğ‘¦  í•¨ìˆ˜ ì‘ì„±
def greedy(Q_table, agent, epsilon):
    pos = agent.get_pos()
    # ìµœì  ë°©í–¥ ë°˜í™˜
    return np.argmax(Q_table[pos[0], pos[1]])


np.random.seed(0)
env = Environment()
agent = Agent()
gamma = 0.9

# ëª¨ë“  ğ‘ âˆˆğ‘†,ğ‘âˆˆğ´(ğ‘†)ì— ëŒ€í•´ ì´ˆê¸°í™”:
# ğ‘„1(ğ‘ ,ğ‘), ğ‘„2(ğ‘ ,ğ‘) â† ì„ì˜ì˜ ê°’
Q1_table = np.random.rand(env.reward.shape[0], env.reward.shape[1], len(agent.action))
Q2_table = np.random.rand(env.reward.shape[0], env.reward.shape[1], len(agent.action))

# Q(ğ‘¡ğ‘’ğ‘Ÿğ‘šğ‘–ğ‘›ğ‘ğ‘™âˆ’ğ‘ ğ‘¡ğ‘ğ‘¡ğ‘’,ğ‘)=0
Q1_table[-1, -1, :] = 0
Q2_table[-1, -1, :] = 0

max_episode = 10000
max_step = 10

print("start Double Q-learning")
alpha = 0.1
epsilon = 0.3

# ê° ì—í”¼ì†Œë“œì— ëŒ€í•´ ë°˜ë³µ :
for epi in tqdm(range(max_episode)):
    dleta = 0
    # S ë¥¼ ì´ˆê¸°í™”
    i = 0
    j = 0
    agent.set_pos([i, j])

    # ì—í”¼ì†Œë“œì˜ ê° ìŠ¤í…ì— ëŒ€í•´ ë°˜ë³µ :
    for k in range(max_step):
        pos = agent.get_pos()

        # ğ‘„1ê³¼ ğ‘„2ë¡œ ë¶€í„° aë¥¼ ì„ íƒ (ì˜ˆ : ğœ–âˆ’ğ‘”ğ‘Ÿğ‘’ğ‘’ğ‘‘ğ‘¦ in ğ‘„1+ğ‘„2)
        Q = Q1_table + Q2_table
        action = e_greedy(Q, agent, epsilon)

        # í–‰ë™ a ë¥¼ ì·¨í•œ í›„ ë³´ìƒ rê³¼ ë‹¤ìŒ ìƒíƒœ s'ë¥¼ ê´€ì¸¡
        observation, reward, done = env.move(agent, action)

        # s' ì—ì„œ íƒ€ê¹ƒ ì •ì±…(Target policy)ìœ¼ë¡œ í–‰ë™ a'ë¥¼ ì„ íƒ(greedy)
        p = np.random.random()

        # í™•ë¥ ì´ 0.5ë³´ë‹¤ ì‘ë‹¤ë©´
        if p < 0.5:
            next_action = greedy(Q1_table, agent, epsilon)

            # ğ‘„1(ğ‘ ,ğ‘) â† ğ‘„1(ğ‘ ,ğ‘) + Î±[ğ‘… + ğ›¾ğ‘„2(ğ‘†', argmaxQ1(s', a')) âˆ’ ğ‘„1 (ğ‘ ,ğ‘)]
            Q1_table[pos[0], pos[1], action] += alpha * (
                reward
                + gamma * Q2_table[observation[0], observation[1], next_action]
                - Q1_table[pos[0], pos[1], action]
            )
        else:
            next_action = greedy(Q2_table, agent, epsilon)

            # ğ‘„2(ğ‘ ,ğ‘) â† ğ‘„2(ğ‘ ,ğ‘) + Î±[ğ‘… + ğ›¾ğ‘„1(ğ‘†', argmaxQ2(s', a')) âˆ’ ğ‘„2(ğ‘ ,ğ‘)]
            Q2_table[pos[0], pos[1], action] += alpha * (
                reward
                + gamma * Q1_table[observation[0], observation[1], next_action]
                - Q2_table[pos[0], pos[1], action]
            )

        # sê°€ ë§ˆì§€ë§‰ ìƒíƒœë¼ë©´ ì¢…ë£Œ
        if done:
            break

# í•™ìŠµëœ ì •ì±… ì¤‘ ìµœì  í–‰ë™ì„ optimal_policyì— ì €ì¥
optimal_policy = np.zeros((env.reward.shape[0], env.reward.shape[1]))
for i in range(env.reward.shape[0]):
    for j in range(env.reward.shape[1]):
        optimal_policy[i, j] = np.argmax(Q1_table[i, j, :] + Q1_table[i, j, :])

print("Double Q-learning : Q1(s,a)")
show_q_table(np.round(Q1_table, 2), env)
print("Double Q-learning : Q2(s,a)")
show_q_table(np.round(Q2_table, 2), env)
print("Double Q-learning : Q(s,a)")
show_q_table(np.round(Q1_table + Q2_table, 2), env)
print("Double Q-learning : optimal policy")
show_policy(optimal_policy, env)
