import numpy as np
import copy
import time
from r01_env_agent import Environment, Agent
from r02_show import show_v_table


# ë°˜ë³µ ì •ì±… í‰ê°€
np.random.seed(0)
env = Environment()
agent = Agent()
gamma = 0.9

# 1. ëª¨ë“  ğ‘  âˆˆ ğ‘†^ì— ëŒ€í•´ì„œ ë°°ì—´ ğ‘‰(ğ‘ )=0ìœ¼ë¡œ ì´ˆê¸°í™”
v_table = np.zeros(
    (
        env.reward.shape[0],
        env.reward.shape[1],
    )
)

print("start Iterative Policy Evaluation")

k = 1
print()
print("V0(S)   k = 0")

# ì´ˆê¸°í™” ëœ V í…Œì´ë¸” ì¶œë ¥
show_v_table(np.round(v_table, 2), env)

# ì‹œì‘ ì‹œê°„ ë³€ìˆ˜ì— ì €ì¥
start_time = time.time()

# ë°˜ë³µ
while True:
    # 2. ğ›¥ <- 0
    delta = 0

    # 3. v <- V(s)
    # ê³„ì‚° ì „ ê°€ì¹˜ ì €ì¥
    temp_v = copy.deepcopy(v_table)

    # 4. ëª¨ë“  sì— ëŒ€í•´
    for i in range(env.reward.shape[0]):
        for j in range(env.reward.shape[1]):
            G = 0

            # 5. ê°€ëŠ¥í•œ ëª¨ë“  í–‰ë™ìœ¼ë¡œ ë‹¤ìŒ ìƒíƒœë§Œ ì´ìš©í•´ V(s) ê³„ì‚° -> ì—°ê²°ëœ ëª¨ë“  ìƒíƒœì˜ ê°€ì¹˜ë¥¼ ë”í•¨
            for action in range(len(agent.action)):
                agent.set_pos([i, j])
                observation, reward, done = env.move(agent, action)

                G += agent.select_action_pr[action] * (
                    reward + gamma * v_table[observation[0], observation[1]]
                )

            v_table[i, j] = G

    # 6. ğ›¥ <- max(ğ›¥, | v - V(s) |
    # ê³„ì‚° ì „ê³¼ ê³„ì‚° í›„ì˜ ê°€ì¹˜ ì°¨ì´ ê³„ì‚°
    delta = np.max([delta, np.max(np.abs(temp_v - v_table))])

    end_time = time.time()
    print(
        "V{0}(S) : k = {1:3d}    delta = {2:0.6f} total_time = {3}".format(
            k, k, delta, np.round(end_time - start_time), 2
        )
    )

    show_v_table(np.round(v_table, 2), env)

    k += 1

    # 7. ğ›¥ < thetaê°€ ì‘ì€ ì–‘ìˆ˜ì¼ ë•Œê¹Œì§€ ë°˜ë³µ
    if delta < 0.000001:
        break

end_time = time.time()
print("total_time = {}".format(np.round(end_time - start_time), 2))
