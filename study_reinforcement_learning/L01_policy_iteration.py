# ì •ì±… ë°˜ë³µ

import random
from L01_environment import GraphicDisplay, Env


class PolicyIteration:
    def __init__(self):
        self.env = Env()

        # ê°€ì¹˜í•¨ìˆ˜ : 2ì°¨ì› ë¦¬ìŠ¤íŠ¸ë¡œ ì´ˆê¸°í™”. width=5, height=5 => 5 x 5
        self.value_table = [[0.0] * self.env.width for _ in range(self.env.height)]
        # ìƒ í•˜ ì¢Œ ìš°ë¥¼ ë™ì¼í•œ í™•ë¥ ë¡œ ì´ˆê¸°í™”. 5 x 5 x 4
        self.policy_table = [
            [[0.25, 0.25, 0.25, 0.25]] * self.env.width for _ in range(self.env.height)
        ]

        # ì¢…ë£Œ ìƒíƒœ ì„¤ì •
        self.policy_table[2][2] = []
        # ê°ê°€ìœ¨
        self.discount_factor = 0.9

    def get_policy(self, state):
        """ìƒíƒœì— ë”°ë¥¸ ì •ì±…(í™•ë¥ ) ë°˜í™˜. state=ì¢Œí‘œ"""
        if state == [2, 2]:
            return 0.0
        return self.policy_table[state[0]][state[1]] # [0.25, 0.25, 0.25, 0.25]

    def get_action(self, state):
        """íŠ¹ì • ìƒíƒœì—ì„œ ì •ì±…ì— ë”°ë¥¸ í–‰ë™ ë°˜í™˜. state=ì¢Œí‘œ"""
        # 0-1 ì‚¬ì´ ê°’ì„ ë¬´ì‘ìœ„ ì¶”ì¶œ. ì†Œìˆ«ì  ë‘ìë¦¬ìˆ˜ê¹Œì§€ë§Œ ë½‘ìœ¼ë ¤ê³  ì•„ë˜ ê°™ì€ ìˆ˜ì‹ ì‚¬ìš©
        random_pick = random.randrange(100) / 100
        policy = self.get_policy(state)  # [0.25, 0.25, 0.25, 0.25]
        policy_sum = 0.0
        # ì •ì±…ì— ë‹´ê¸´ í–‰ë™ ì¤‘ ë¬´ì‘ìœ„ë¡œ í•œ í–‰ë™ ì¶”ì¶œ
        for index, value in enumerate(policy):
            policy_sum += value
            if random_pick < policy_sum:
                return index

    def get_value(self, state):
        """ê°€ì¹˜í•¨ìˆ˜ ê°’ ë°˜í™˜"""
        return round(self.value_table[state[0]][state[1]], 2)

    def policy_evaluation(self):
        """ì •ì±…í‰ê°€: ë²¨ë§Œ ê¸°ëŒ€ ë°©ì •ì‹ì„ í†µí•´ ë‹¤ìŒ ê°€ì¹˜í•¨ìˆ˜ë¥¼ ê³„ì‚°"""
        
        # ì—…ë°ì´íŠ¸ ì •ë³´ë¥¼ ë‹´ì„ ìƒíƒœê°€ì¹˜ ë³€ìˆ˜ ì´ˆê¸°í™” (5,5)
        next_value_table = [[0.00] * self.env.width for _ in range(self.env.height)]

        # ëª¨ë“  ìƒíƒœì— ëŒ€í•´ ë²¨ë§Œ ê¸°ëŒ€ ë°©ì •ì‹ ê³„ì‚° 
        # ëª¨ë“  ìƒíƒœ env.get_all_states() => [[0,0], [0,1], ..., [4,4]]
        for state in self.env.get_all_states():
            value = 0.0
            # ì¢…ë£Œ ìƒíƒœì˜ ê°€ì¹˜ = 0
            if state == [2, 2]:
                next_value_table[state[0]][state[1]] = 0.0
                continue    # [2,2] ìƒíƒœì—ì„œëŠ” ì•„ë˜ ì½”ë“œ ì‹¤í–‰í•˜ì§€ ì•Šê³  ë„˜ì–´ê°
            
            # ë²¨ë§Œ ê¸°ëŒ€ ë°©ì •ì‹ -> í˜„ì¬ ìƒíƒœì˜ ëª¨ë“  í–‰ë™ìœ¼ë¡œ ì–»ëŠ” ê°€ì¹˜ í•©ê³„
            for action in self.env.possible_actions:
                # í˜„ì¬ ìƒíƒœì—ì„œ í–‰ë™ì„ í–ˆì„ ë•Œì˜ ë³´ìƒ
                reward = self.env.get_reward(state, action)
                # í–‰ë™ í›„ì˜ ìƒíƒœ ì¢Œí‘œ eg. [1, 2]
                next_state = self.env.state_after_action(state, action)
                # ì´ë™í•œ ìƒíƒœì˜ ê°€ì¹˜
                next_value = self.get_value(next_state)
                # ğšº <- ğ›‘(a|s) * [R_t+1 + ğ›„ v(s')]
                value += self.get_policy(state)[action] * (
                    reward + self.discount_factor * next_value
                )
            # íŠ¹ì • ìƒíƒœì˜ ëª¨ë“  í–‰ë™ìœ¼ë¡œ ì–»ì€ ê°€ì¹˜ í•©ê³„ë¥¼ "ì—…ë°ì´íŠ¸ ìš© ìƒíƒœê°€ì¹˜"ì— ì €ì¥
            next_value_table[state[0]][state[1]] = round(value, 2)
        
        # ìƒíƒœê°€ì¹˜ ì—…ë°ì´íŠ¸
        self.value_table = next_value_table

    def policy_improvement(self):
        """í˜„ì¬ ê°€ì¹˜í•¨ìˆ˜ì— ëŒ€í•´ íƒìš•ì •ì±…ê°œì„ """
        # ì—…ë°ì´íŠ¸ ìš© í–‰ë™ê°€ì¹˜ ë³€ìˆ˜ (5,5,4)
        next_policy = self.policy_table
        # ëª¨ë“  ìƒíƒœì— ëŒ€í•´ ë²¨ë§Œ ìµœì  ë°©ì •ì‹ ì ìš©
        for state in self.env.get_all_states():
            if state == [2, 2]:
                continue
            value = -99999   # ìµœëŒ€ê°’ íŒë³„ ìš©
            max_index = []   # ë°›ì„ ë³´ìƒì´ ìµœëŒ€ì¸ í–‰ë™ ì €ì¥
            # ë°˜í™˜í•  ì •ì±… ì´ˆê¸°í™”
            result = [0.0, 0.0, 0.0, 0.0]
            # ëª¨ë“  í–‰ë™ì— ëŒ€í•´ [ë³´ìƒ + (ê°ê°€ìœ¨ * ë‹¤ìŒ ìƒíƒœ ê°€ì¹˜í•¨ìˆ˜)] ê³„ì‚°
            for index, action in enumerate(self.env.possible_actions):
                reward = self.env.get_reward(state, action)
                next_state = self.env.state_after_action(state, action)
                next_value = self.get_value(next_state)
                temp = reward + self.discount_factor * next_value
                # ë°›ì„ ë³´ìƒì´ ìµœëŒ€ì¸ í–‰ë™ì˜ ì¸ë±ìŠ¤(ì—¬ëŸ¬ ê°œë¼ë©´ ëª¨ë‘) ì¶”ì¶œ
                if temp == value:           # í˜„ì¬ í–‰ë™ì˜ ê°€ì¹˜ê°€ ìµœëŒ“ê°’ê³¼ ê°™ìœ¼ë©´
                    max_index.append(index)
                elif temp > value:
                    value = temp
                    max_index.clear()
                    max_index.append(index)
            # ìµœëŒ€ê°’ì´ ì—¬ëŸ¬ ê°œì¼ ë•Œ í™•ë¥ ì„ ë˜‘ê°™ì´ ë§Œë“¦
            prob = 1 / len(max_index) # 1ê°œì´ë©´ 1, 2ê°œì´ë©´ 0.5
            for index in max_index:
                result[index] = prob
            # í–‰ë™ê°€ì¹˜ ì €ì¥
            next_policy[state[0]][state[1]] = result  # ìµœëŒ“ê°’ì˜ í™•ë¥ ë§Œ ì €ì¥, ë‚˜ë¨¸ì§€ëŠ” ê¸°ë³¸ê°’ 0
        # í–‰ë™ ê°€ì¹˜ ì—…ë°ì´íŠ¸
        self.policy_table = next_policy  # ìµœëŒ“ê°’ 1ê°œ -> eg. [0, 0, 1, 0], 2ê°œ -> eg. [0.5, 0, 0, 0.5]


if __name__ == "__main__":
    policy_iteration = PolicyIteration()
    grid_world = GraphicDisplay(policy_iteration)
    grid_world.mainloop()
