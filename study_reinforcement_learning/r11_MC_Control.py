# MC Control

import numpy as np
from tqdm import tqdm

from r01_env_agent import Environment, Agent
from r02_show import show_q_table, show_policy


# ì •ì±…ì„ ë°›ì•„ ì—í”¼ì†Œë“œ ìƒì„±
def generate_episode_with_policy(env, agent, first_visit, policy):
    gamma = 0.09

    # ì—í”¼ì†Œë“œ ì €ì¥ ë¦¬ìŠ¤íŠ¸
    episode = []

    # ì´ì „ ë°©ë¬¸ ì—¬ë¶€ ì²´í¬
    visit = np.zeros((env.reward.shape[0], env.reward.shape[1], len(agent.action)))

    # ì—ì´ì „íŠ¸ëŠ” í•­ìƒ (0,0)ì—ì„œ ì¶œë°œ
    i = 0
    j = 0
    agent.set_pos([i, j])

    # ì—í”¼ì†Œë“œ ìˆ˜ìµ ì´ˆê¸°í™”
    G = 0

    # ê°ì‡„ìœ¨ ì§€ìˆ˜
    step = 0
    max_step = 100

    # ì—í”¼ì†Œë“œ ìƒì„±
    for k in range(max_step):
        pos = agent.get_pos()

        # í˜„ì¬ ìƒíƒœì˜ ì •ì±…ì„ ì´ìš©í•´ í–‰ë™ì„ ì„ íƒí•œ í›„ ì´ë™
        action = np.random.choice(
            range(0, len(agent.action)), p=policy[pos[0], pos[1], :]
        )

        observation, reward, done = env.move(agent, action)

        if first_visit:
            # ì—í”¼ì†Œë“œê°€ ì²« ë°©ë¬¸ì¸ì§€ ê²€ì‚¬
            if visit[pos[0], pos[1], action] == 0:
                # ì—í”¼ì†Œë“œê°€ ëë‚  ë•Œê¹Œì§€ G ê³„ì‚°
                G += gamma**step * reward
                # ë°©ë¬¸ ì´ë ¥ í‘œì‹œ
                visit[pos[0], pos[1], action] = 1
                step += 1
                # ë°©ë¬¸ ì´ë ¥ ì €ì¥(ìƒíƒœ, í–‰ë™, ë³´ìƒ)
                episode.append((pos, action, reward))

        else:
            # every-visit MC
            G += gamma**step * reward
            step += 1
            episode.append((pos, action, reward))

        # ì—í”¼ì†Œë“œê°€ ì¢…ë£Œë˜ë©´(done==True) ë£¨í”„ì—ì„œ íƒˆì¶œ
        if done:
            break

    return i, j, G, episode


# ëª¬í…Œì¹´ë¥¼ë¡œ ë°©ë²•ì˜ Control ì•Œê³ ë¦¬ì¦˜
np.random.seed(0)

# í™˜ê²½, ì—ì´ì „íŠ¸ ì´ˆê¸°í™”
env = Environment()
agent = Agent()

# ëª¨ë“  ğ‘ âˆˆğ‘†,ğ‘âˆˆğ´(ğ‘†)ì— ëŒ€í•´ ì´ˆê¸°í™”:
# # ğ‘„(ğ‘ ,ğ‘)â†ì„ì˜ì˜ ê°’ (í–‰ë™ ê°œìˆ˜, ë¯¸ë¡œ ì„¸ë¡œ, ë¯¸ë¡œ ê°€ë¡œ)
Q_table = np.random.rand(env.reward.shape[0], env.reward.shape[1], len(agent.action))

print("Initial Q(s, a)")
show_q_table(Q_table, env)

# ìƒíƒœ ë°©ë¬¸ íšŸìˆ˜ ì €ì¥ í…Œì´ë¸”
Q_visit = np.zeros((env.reward.shape[0], env.reward.shape[1], len(agent.action)))

# ë¯¸ë¡œì˜ ëª¨ë“  ìƒíƒœì—ì„œì˜ ìµœì  í–‰ë™ ì €ì¥ í…Œì´ë¸”
optimal_a = np.zeros((env.reward.shape[0], env.reward.shape[1]))

# ê° ìƒíƒœì—ì„œ Qê°’ì´ ê°€ì¥ í° í–‰ë™ ì„ íƒ í›„ optimal_aì— ì €ì¥
for i in range(env.reward.shape[0]):
    for j in range(env.reward.shape[1]):
        optimal_a[i, j] = np.argmax(Q_table[i, j, :])
print("initial optimal_a")
show_policy(optimal_a, env)

# Ï€(ğ‘ ,ğ‘)â†ì„ì˜ì˜ ğœ–âˆ’íƒìš• ì •ì±…
# ë¬´ì‘ìœ„ë¡œ í–‰ë™ì„ ì„ íƒí•˜ë„ë¡ ì§€ì •
policy = np.zeros((env.reward.shape[0], env.reward.shape[1], len(agent.action)))

# í•œ ìƒíƒœì—ì„œì˜ ê°€ëŠ¥í•œ í™•ë¥ ì˜ í•©ì´ 1ì´ ë˜ë„ë¡
# ğœ–âˆ’íƒìš• ì •ì±…ìœ¼ë¡œ ì •ì±… ê³„ì‚°
epsilon = 0.8
for i in range(env.reward.shape[0]):
    for j in range(env.reward.shape[1]):
        for k in range(len(agent.action)):
            if optimal_a[i, j] == k:  # actionì´ ìµœì ì´ë©´
                policy[i, j, k] = 1 - epsilon + epsilon / len(agent.action)
            else:  # actionì´ ìµœì ì´ ì•„ë‹ˆë©´
                policy[i, j, k] = epsilon / len(agent.action)

print("Initail Policy")
show_q_table(policy, env)

# ìµœëŒ€ ì—í”¼ì†Œë“œ ìˆ˜ ì§€ì •
max_episode = 10000

first_visit = True
if first_visit:
    print("start first visit MC")
else:
    print("start every visit MC")
print()

gamma = 0.09

# ì—í”¼ì†Œë“œë¥¼ ì´ìš©í•´ Q_tableì˜ ê° ì •ì±…(ë°©í–¥) ê°’ ì—…ë°ì´íŠ¸
for epi in tqdm(range(max_episode)):
    # (a) ğ›‘ë¥¼ ì´ìš©í•´ ì—í”¼ì†Œë“œ 1ê°œ ìƒì„±
    x, y, G, episode = generate_episode_with_policy(env, agent, first_visit, policy)

    for step_num in range(len(episode)):
        G = 0
        i = episode[step_num][0][0]
        j = episode[step_num][0][1]
        action = episode[step_num][1]

        # ì—í”¼ì†Œë“œ ì‹œì‘ì  ì¹´ìš´íŠ¸
        Q_visit[i, j, action] += 1

        # ì„œë¸Œ ì—í”¼ì†Œë“œ(episode[step_num:])ì˜ ì¶œë°œë¶€í„° ëê¹Œì§€ ìˆ˜ìµ G ê³„ì‚°
        # k[2] = episode[step_num][2] = step_numë²ˆì§¸ ë°›ì€ ë³´ìƒ
        for step, k in enumerate(episode[step_num:]):
            G += gamma**step * k[2]

        # Incremental mean: Q(s,a) <- average(Return(s,a))
        Q_table[i, j, action] += 1 / Q_visit[i, j, action] * (G - Q_table[i, j, action])

    # optimal_a(ìµœì  ì •ì±…) ì—…ë°ì´íŠ¸
    # by ì—…ë°ì´íŠ¸ ëœ Q_table ê° sì˜ ë„¤ ë°©í–¥ ì¤‘ ê°€ì¥ ë†’ì€ ê°’ì„ optimal_a(ìµœì  ì •ì±…)ì— ì €ì¥
    # (c) ì—í”¼ì†Œë“œ ì•ˆì˜ ê° sì— ëŒ€í•´ì„œ
    # ë¯¸ë¡œ ëª¨ë“  ìƒíƒœì—ì„œ ìµœì  í–‰ë™ì„ ì €ì¥í•  ê³µê°„ ë§ˆë ¨
    # ğ‘âˆ— â†argmax_a ğ‘„(ğ‘ ,ğ‘)
    for i in range(env.reward.shape[0]):
        for j in range(env.reward.shape[1]):
            optimal_a[i, j] = np.argmax(Q_table[i, j, :])

    # ëª¨ë“  ğ‘âˆˆğ´(ğ‘†) ì— ëŒ€í•´ì„œ :
    # ìƒˆë¡œ ê³„ì‚°ëœ optimal_a ë¥¼ ì´ìš©í•´ì„œ í–‰ë™ ì„ íƒ í™•ë¥  policy (Ï€) ê°±ì‹ 
    epsilon = 1 - epi / max_episode  # TODO ğœ– ê°’ì„ ë³€ê²½í•˜ëŠ” ì´ìœ ????

    for i in range(env.reward.shape[0]):
        for j in range(env.reward.shape[1]):
            for k in range(len(agent.action)):
                if optimal_a[i, j] == k:
                    policy[i, j, k] = 1 - epsilon + epsilon / len(agent.action)
                else:
                    policy[i, j, k] = epsilon / len(agent.action)

print("Final Q(s,a)")
show_q_table(Q_table, env)
print("Final policy")
show_q_table(policy, env)
print("Final optimal_a")
show_policy(optimal_a, env)
