# finding optimal value

import copy
import numpy as np
import time
from r01_env_agent import Environment, Agent
from r02_show import show_policy, show_v_table


# ìµœì  ìƒíƒœê°€ì¹˜ë¥¼ ì°¾ëŠ” í•¨ìˆ˜
def finding_optimal_value(env, agent, v_table):
    k = 0
    gamma = 0.9
    while True:
        delta = 0
        #  v â† ğ‘‰(ğ‘ )
        temp_v = copy.deepcopy(v_table)

        # ëª¨ë“  ğ‘  âˆˆ ğ‘† ì— ëŒ€í•´
        for i in range(env.reward.shape[0]):
            for j in range(env.reward.shape[1]):
                temp = -1e10

                # ê°€ëŠ¥í•œ í–‰ë™ ì¤‘ ìµœëŒ“ê°’ ì°¾ê¸°
                # # ğ‘‰(ğ‘ )â† max(a) âˆ‘ğ‘ƒ(ğ‘ '|ğ‘ ,ğ‘)[ğ‘Ÿ(ğ‘ ,ğ‘,ğ‘ ') +ğ›¾ğ‘‰(ğ‘ ')]
                for action in range(len(agent.action)):
                    agent.set_pos([i, j])
                    observation, reward, done = env.move(agent, action)

                    # ì´ë™í•œ ìƒíƒœì˜ ê°€ì¹˜ê°€ tempë³´ë‹¤ í¬ë©´
                    if temp < reward + gamma * v_table[observation[0], observation[1]]:
                        temp = reward + gamma * v_table[observation[0], observation[1]]

                # ì´ë™ ê°€ëŠ¥í•œ ìƒíƒœ ì¤‘ ê°€ì¥ í° ê°€ì¹˜ ì €ì¥
                v_table[i, j] = temp

        # ì´ì „ ê°€ì¹˜ì™€ ë¹„êµí•´ í° ê°’ì„ deltaì— ì €ì¥
        # ê³„ì‚° ì „ê³¼ ê³„ì‚° í›„ì˜ ê°€ì¹˜ ì°¨ì´ ê³„ì‚° âˆ† â† max(âˆ†,|vâˆ’ğ‘‰(ğ‘ )|)
        delta = np.max([delta, np.max(np.abs(temp_v - v_table))])

        # deltaê°€ ì‘ì€ ì–‘ìˆ˜ì¼ ë•Œê¹Œì§€ ë°˜ë³µ
        if delta < 0.0000001:
            break

        print("V{0}(S) : k = {1:3d}    delta = {2:0.6f}".format(k, k, delta))
        show_v_table(np.round(v_table, 2), env)
        k += 1

    return v_table


# ì •ì±…ì„ ì¶”ì¶œí•˜ëŠ” í•¨ìˆ˜
def policy_extraction(env, agent, v_table, optimal_policy):
    gamma = 0.9

    # ì •ì±… ğœ‹ë¥¼ ë‹¤ìŒê³¼ ê°™ì´ ì¶”ì¶œ
    # ğœ‹(ğ‘ )â† argmax(a) âˆ‘ğ‘ƒ(ğ‘ '|ğ‘ ,ğ‘)[ğ‘Ÿ(ğ‘ ,ğ‘,ğ‘ ') +ğ›¾ğ‘‰(ğ‘ ')]
    # ëª¨ë“  ğ‘ âˆˆğ‘†ì— ëŒ€í•´
    for i in range(env.reward.shape[0]):
        for j in range(env.reward.shape[1]):
            temp = -1e10

            # ê°€ëŠ¥í•œ í–‰ë™ ì¤‘ ê°€ì¹˜ê°€ ê°€ì¥ ë†’ì€ ê°’ì„ policy[i, j]ì— ì €ì¥
            for action in range(len(agent.action)):
                agent.set_pos([i, j])
                observation, reward, done = env.move(agent, action)
                if temp < reward + gamma * v_table[observation[0], observation[1]]:
                    optimal_policy[i, j] = action
                    temp = reward + gamma * v_table[observation[0], observation[1]]
    return optimal_policy


# ê°€ì¹˜ ë°˜ë³µ
np.random.seed(0)
env = Environment()
agent = Agent()

# ì´ˆê¸°í™”
# ëª¨ë“  ğ‘ âˆˆğ‘†^+ì— ëŒ€í•´ ğ‘‰(ğ‘ )âˆˆğ‘…ì„ ì„ì˜ë¡œ ì„¤ì •
v_table = np.random.rand(env.reward.shape[0], env.reward.shape[1])

print("Initial random V0(S)")
show_v_table(np.round(v_table, 2), env)
print()

optimal_policy = np.zeros((env.reward.shape[0], env.reward.shape[1]))

print("start Value iteration")
print()

# ì‹œì‘ ì‹œê°„ ë³€ìˆ˜ì— ì €ì¥
start_time = time.time()

# ìµœì  ìƒíƒœê°€ì¹˜ ê³„ì‚°
v_table = finding_optimal_value(env, agent, v_table)

# ìµœì  ì •ì±… ì¶”ì¶œ
optimal_policy = policy_extraction(env, agent, v_table, optimal_policy)

print("total_time = {}".format(np.round(time.time() - start_time), 2))
print()
print("Optimal policy")
show_policy(optimal_policy, env)
