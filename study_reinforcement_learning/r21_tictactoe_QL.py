# í‹±íƒí†  ê²Œì„ : Q-learning

import numpy as np
from tqdm import tqdm


class Q_learning_player:
    """
    1. ì´ˆê¸°í™”: qtable, epsilon, gamma, learning_rate(alpha)
    2. select_action() : ì •ì±…(policy)ì— ë§ëŠ” í–‰ë™(action) ë°˜í™˜
    3. policy() : ê°€ëŠ¥í•œ í–‰ë™ë“¤ì— e-greedy ë°˜ì˜í•œ í™•ë¥  ë°˜í™˜
    4. learn_qtable() :
    """

    def __init__(self):
        self.name = "Q_player"

        # Q-tableì„ ë”•ì…”ë„ˆë¦¬ë¡œ ì •ì˜
        self.qtable = {}

        # ğœ€-greedy ê³„ìˆ˜ ì •ì˜
        self.epsilon = 1

        # í•™ìŠµë¥  ì •ì˜
        self.learning_rate = 0.1
        self.gamma = 0.9
        self.print = False

    def select_action(self, env, player):
        """policyì— ë”°ë¼ ìƒíƒœì— ë§ëŠ” í–‰ë™ì„ ì„ íƒ"""
        action = self.policy(env)
        if self.print:
            print("{} : select action".format(action))
        return action

    def policy(self, env):
        if self.print:
            print("-----------   policy start -------------")
        # í–‰ë™ ê°€ëŠ¥í•œ ìƒíƒœë¥¼ ì €ì¥
        available_action = env.get_action()
        # í–‰ë™ ê°€ëŠ¥í•œ ìƒíƒœì˜ Q-valueë¥¼ ì €ì¥
        qvalues = np.zeros(len(available_action))

        if self.print:
            print("{} : available_action".format(available_action))

        # í–‰ë™ ê°€ëŠ¥í•œ ìƒíƒœì˜ Q-valueë¥¼ ì¡°ì‚¬
        for i, act in enumerate(available_action):
            key = (tuple(env.board_a), act)

            # í˜„ì¬ ìƒíƒœë¥¼ ê²½í—˜í•œ ì ì´ ì—†ë‹¤ë©´(ë”•ì…”ë„ˆë¦¬ì— ì—†ë‹¤ë©´) ë”•ì…”ë„ˆë¦¬ì— ì¶”ê°€(Q-value = 0)
            if self.qtable.get(key) == None:
                self.qtable[key] = 0
            # í–‰ë™ ê°€ëŠ¥í•œ ìƒíƒœì˜ Q-value ì €ì¥
            qvalues[i] = self.qtable.get(key)

        if self.print:
            for key, val in self.qtable.items():
                print("key = {key}, value={value}".format(key=key, value=val))
            print("policy state_value = {}".format(state_qvalue))
            for key, val in self.qtable1.items():
                print("key = {key}, count={value}".format(key=key, value=val))

        # ğœ€-greedy: ê°€ëŠ¥í•œ í–‰ë™ ì¤‘ Q-valueê°€ ê°€ì¥ í° í–‰ë™ ì €ì¥
        ## max Q-valueê°€ ì—¬ëŸ¬ ê°œë¼ë©´ ê·¸ ì¤‘ì—ì„œ ë‹¤ì‹œ ì„ íƒ
        greedy_action = np.argmax(qvalues)

        pr = np.zeros(len(available_action))
        if self.print:
            print("{} : self.epsilon = {}".format(self.epsilon))
            print("{} : greedy_action".format(greedy_action))
            print("{} : qvalues".format(greedy_action, qvalues[greedy_action]))

        # max Q-valueì™€ ê°™ì€ ê°’ì´ ì—¬ëŸ¬ê°œ ìˆëŠ”ì§€ í™•ì¸í•œ í›„ double_checkì— ìƒíƒœë¥¼ ì €ì¥
        ## ìµœëŒ“ê°’ì€ 1ë¡œ, ë‚˜ë¨¸ì§€ëŠ” 0ìœ¼ë¡œ ë³€í™˜ np.where(ì¡°ê±´, ì°¸, ê±°ì§“)
        double_check = np.where(qvalues == np.max(qvalues), 1, 0)

        #  ì—¬ëŸ¬ê°œ ìˆë‹¤ë©´ ì¤‘ë³µëœ ìƒíƒœì¤‘ì—ì„œ ë‹¤ì‹œ ë¬´ì‘ìœ„ë¡œ ì„ íƒ
        if np.sum(double_check) > 1:
            if self.print:
                print("{} : double_check".format(np.round(double_check, 2)))
            ## ì¤‘ë³µê°’ì„ ëª¨ë‘ ê°™ì€ í™•ë¥ ë¡œ ë³€í™˜
            double_check = double_check / np.sum(double_check)
            greedy_action = np.random.choice(
                range(0, len(double_check)), p=double_check
            )
            if self.print:
                print("{} : greedy_action".format(greedy_action))
                print("{} : double_check".format(np.round(double_check, 2)))
                print("{} : selected state".format(available_state[greedy_action]))

        # ğœ€-greedyë¡œ í–‰ë™ë“¤ì˜ ì„ íƒ í™•ë¥ ì„ ê³„ì‚°
        pr = np.zeros(len(available_action))

        for i in range(len(available_action)):
            if i == greedy_action:
                pr[i] = 1 - self.epsilon + self.epsilon / len(available_action)
                if pr[i] < 0:
                    print("{} : - pr".format(np.round(pr[i], 2)))
            else:
                pr[i] = self.epsilon / len(available_action)
                if pr[i] < 0:
                    print("{} : - pr".format(np.round(pr[i], 2)))

        action = np.random.choice(range(0, len(available_action)), p=pr)

        if self.print:
            print("pr = {}".format(np.round(pr, 2)))
            print("action = {}".format(action))
            print("state[action] = {}".format(state[action]))
            print("-----------   policy end -------------")

        return available_action[action]

    def learn_qtable(self, board_bakup, action_backup, env, reward):
        # í˜„ì¬ ìƒíƒœì™€ í–‰ë™ì„ í‚¤ë¡œ ì €ì¥
        key = (board_bakup, action_backup)
        if self.print:
            print("-----------   learn_qtable start -------------")
            print(
                "{} : board_bakup, {} : action_backup, {} : reward".format(
                    board_bakup, action_backup, reward
                )
            )
            print("{} : key".format(key))
        # Q-table í•™ìŠµ
        if env.done:
            # ê²Œì„ì´ ëë‚¬ì„ ê²½ìš° í•™ìŠµ
            # Q(s, a) <- Q(s, a) + ğ›¼ * [r - Q(s, a)]
            if self.print:
                print("{} : before self.qtable[key]".format(self.qtable[key]))

            self.qtable[key] += self.learning_rate * (reward - self.qtable[key])

            if self.print:
                print("{} : after self.qtable[key]".format(self.qtable[key]))
        else:
            # ê²Œì„ì´ ì§„í–‰ì¤‘ì¼ ê²½ìš° í•™ìŠµ
            # ë‹¤ìŒ ìƒíƒœì˜ max Q ê°’ ê³„ì‚°
            available_action = env.get_action()
            qvalues = np.zeros(len(available_action))

            for i, act in enumerate(available_action):
                next_key = (tuple(env.board_a), act)
                # ë‹¤ìŒ ìƒíƒœë¥¼ ê²½í—˜í•œ ì ì´ ì—†ë‹¤ë©´(ë”•ì…”ë„ˆë¦¬ì— ì—†ë‹¤ë©´) ë”•ì…”ë„ˆë¦¬ì— ì¶”ê°€(Q-value = 0)
                if self.qtable.get(next_key) == None:
                    self.qtable[next_key] = 0
                qvalues[i] = self.qtable.get(next_key)

            # maxQ ì¡°ì‚¬
            maxQ = np.max(qvalues)

            if self.print:
                print("{} : before self.qtable[key]".format(self.qtable[key]))

            # ê²Œì„ì´ ì§„í–‰ì¤‘ì¼ ë•Œ í•™ìŠµ
            # Q(s, a) <- Q(s, a) + ğ›¼ * [r + ğ›¾ * maxQ(s', a') - Q(s, a)]
            self.qtable[key] += self.learning_rate * (
                reward + self.gamma * maxQ - self.qtable[key]
            )

            if self.print:
                print("{} : after self.qtable[key]".format(self.qtable[key]))

        if self.print:
            print("-----------   learn_qtable end -------------")


class Environment:
    def __init__(self):
        """
        - í™”ë©´ì— o, xë¥¼ í‘œì‹œí•˜ëŠ” ë³´ë“œ, ê³„ì‚°ì— ì‚¬ìš©ë˜ëŠ” ë³´ë“œ
        - ë³´ë“œëŠ” 0ìœ¼ë¡œ ì´ˆê¸°í™”ëœ 3x3 = 9 í¬ê¸°ì˜ ë°°ì—´
        - ê²Œì„ ì¢…ë£Œ : done = True
        """
        self.board_a = np.zeros(9)
        self.done = False
        self.reward = 0  # ìŠ¹ì(1 or -1), ë¬´ìŠ¹ë¶€(0)
        # self.winner = 0
        self.print = False  # ê¸°ë³¸ì€ ìë™ëª¨ë“œ

    def print_board(self):
        """
        - í˜„ì¬ ë³´ë“œ ìƒíƒœ ì¶œë ¥: p1 = O, p2 = X
        - ì„ ê³µ p1, í›„ê³µ p2
        """
        print("+----+----+----+")
        for i in range(3):
            for j in range(3):
                # [0], 1, 2,
                # [3], 4, 5,
                # [6], 7, 8
                if self.board_a[3 * i + j] == 1:
                    print("| 0 ", end=" ")
                elif self.board_a[3 * i + j] == -1:
                    print("| X ", end=" ")
                else:
                    print("|   ", end=" ")
            print("|")
            print("+----+----+----+")

    def get_action(self):
        """
        í˜„ì¬ ë³´ë“œ ìƒíƒœì—ì„œ ê°€ëŠ¥í•œ í–‰ë™(ë‘˜ ìˆ˜ ìˆëŠ” ì¥ì†Œ)ì„ íƒìƒ‰í•˜ê³  ë¦¬ìŠ¤íŠ¸ë¡œ ë°˜í™˜
        """
        observation = []
        for i in range(9):
            if self.board_a[i] == 0:
                observation.append(i)  # ë¹ˆ ë³´ë“œì¹¸ì˜ ì¸ë±ìŠ¤ ì¶”ê°€
        return observation

    def end_check(self, player):
        """
        - ê²Œì„ ì¢…ë£Œ(ìŠ¹íŒ¨ or ë¹„ê¹€) íŒë‹¨
        - ìŠ¹íŒ¨ ì¡°ê±´ì€ ê°€ë¡œ, ì„¸ë¡œ, ëŒ€ê°ì„ ì´ -1ì´ë‚˜ 1ë¡œ ë™ì¼í•  ë•Œ
        """
        end_condition = (
            (0, 1, 2),
            (3, 4, 5),
            (6, 7, 8),  # ê°€ë¡œ
            (0, 3, 6),
            (1, 4, 7),
            (2, 5, 8),  # ì„¸ë¡œ
            (0, 4, 8),
            (2, 4, 6),  # ëŒ€ê°ì„ 
        )
        # end_condition ë°°ì—´ì„ ëŒë©°
        # ì²« ë²ˆì§¸ ì—´ == ë‘ ë²ˆì§¸ ì—´ (0 == 1), (3 == 4), (6 == 7)
        # ë‘ ë²ˆì§¸ ì—´ == ì„¸ ë²ˆì§¸ ì—´ (1 == 2), (4 == 5), (7 == 8)
        # ì²´í¬í•˜ëŠ” ë³´ë“œì¹¸ì´ ë¹„ì–´ìˆì§€ ì•Šì•„ì•¼ í•¨ => ê°€ë¡œ ì¼ì¹˜
        for line in end_condition:
            if (
                self.board_a[line[0]] == self.board_a[line[1]]
                and self.board_a[line[1]] == self.board_a[line[2]]
                and self.board_a[line[0]] != 0
            ):
                # ì¢…ë£Œ ìƒíƒœ, ìŠ¹ì ì €ì¥ í›„ í•¨ìˆ˜ ì¢…ë£Œ
                self.done = True
                self.reward = player
                return

        # ë¹„ê¸´ ìƒíƒœ : ë³´ë“œì— ë¹ˆ ê³µê°„ì´ ì—†ì„ ë•Œ
        observation = self.get_action()
        if len(observation) == 0:
            self.done = True  # ì¢…ë£Œ ìƒíƒœ
            self.reward = 0  # ë¬´ìŠ¹ë¶€
        # return

    def move(self, p1, p2, player):
        """
        # ê²Œì„ ì§„í–‰
        - ê° í”Œë ˆì´ì–´ ì„ íƒ í–‰ë™ í‘œì‹œí•˜ê³  ê²Œì„ ìƒíƒœ(ì§„í–‰ or ì¢…ë£Œ) íŒë‹¨
        - p1 = 1, p2 = -1ë¡œ ì •ì˜
        - ê° í”Œë ˆì´ì–´ëŠ” í–‰ë™ì„ ì„ íƒí•˜ëŠ” select_action ë©”ì„œë“œ ê°€ì§
        """
        if player == 1:
            pos = p1.select_action(env, player)
        else:
            pos = p2.select_action(env, player)

        # ë³´ë“œì— í”Œë ˆì´ì–´ì˜ ì„ íƒ í‘œì‹œ
        self.board_a[pos] = player
        if self.print:  # ìˆ˜ë™ ëª¨ë“œì¼ ê²½ìš°ì—ë§Œ ì¶œë ¥
            print(player)
            self.print_board()

        # ê²Œì„ ì¢…ë£Œ ìƒíƒœ ì—¬ë¶€ íŒë‹¨
        self.end_check(player)
        return self.reward, self.done


class Human_player:
    """ì¸ê°„ í”Œë ˆì´ì–´"""

    def __init__(self):
        self.name = "Human player"

    def select_action(self, env, player):
        while True:
            # ê°€ëŠ¥í•œ í–‰ë™ ì¡°ì‚¬ í›„ í‘œì‹œ
            available_action = env.get_action()
            print("possible actions = {}".format(available_action))

            # ìƒíƒœ ë²ˆí˜¸ ì•ˆë‚´ ì¶œë ¥
            print("+----+----+----+")
            print("+  0 +  1 +  2 +")
            print("+----+----+----+")
            print("+  3 +  4 +  5 +")
            print("+----+----+----+")
            print("+  6 +  7 +  8 +")
            print("+----+----+----+")

            # í‚¤ë³´ë“œë¡œ ê°€ëŠ¥í•œ í–‰ë™ ì…ë ¥ë°›ìŒ
            action = input("Select action(human) : ")
            action = int(action)

            # ì…ë ¥ë°›ì€ í–‰ë™ì´ ê°€ëŠ¥í•œ í–‰ë™ì´ë©´ ë°˜ë³µë¬¸ íƒˆì¶œ
            if action in available_action:
                return action

            # ì•„ë‹ˆë©´ í–‰ë™ ì…ë ¥ ë°˜ë³µ
            else:
                print("You selected wrong action")


# ê²Œì„ ì§„í–‰
p1_Qplayer = Q_learning_player()
p2_Qplayer = Q_learning_player()

p1_Qplayer.epsilon = 0.5
p2_Qplayer.epsilon = 0.5
p1_score = 0
p2_score = 0
draw_score = 0
max_learn = 100000

for j in tqdm(range(max_learn)):
    np.random.seed(j)
    env = Environment()
    for i in range(10000):
        # p1 í–‰ë™ ì„ íƒ(p1ì´ ë‘˜ ê³³ ì„ íƒ)
        player = 1
        pos = p1_Qplayer.policy(env)

        # í˜„ì¬ ìƒíƒœ s, í–‰ë™ a ì €ì¥
        p1_board_backup = tuple(env.board_a)
        p1_action_backup = pos
        env.board_a[pos] = player  # ë³´ë“œì— ì°©ìˆ˜
        env.end_check(player)  # ê²Œì„ ì¢…ë£Œ ì²´í¬

        # ê²Œì„ì´ ì¢…ë£Œ ìƒíƒœë¼ë©´ ê° í”Œë ˆì´ì–´ì˜ Q-table í•™ìŠµ
        if env.done:
            # ë¹„ê²¼ìœ¼ë©´ ë³´ìƒ 0
            if env.reward == 0:
                p1_Qplayer.learn_qtable(p1_board_backup, p1_action_backup, env, 0)
                p2_Qplayer.learn_qtable(p2_board_backup, p2_action_backup, env, 0)
                draw_score += 1
                break
            else:
                p1_Qplayer.learn_qtable(
                    p1_board_backup, p1_action_backup, env, 1
                )  # p1 ìŠ¹ë¦¬
                p2_Qplayer.learn_qtable(
                    p2_board_backup, p2_action_backup, env, -1
                )  # p2 íŒ¨ë°°
                p1_score += 1
                break

        # ê²Œì„ì´ ëë‚˜ì§€ ì•Šì•˜ë‹¤ë©´ p2ì˜ Q-tableì„ í•™ìŠµ (ê²Œì„ ì‹œì‘ì§í›„ì—ëŠ” p2ëŠ” í•™ìŠµí• ìˆ˜ ì—†ìŒ)
        if i != 0:
            p2_Qplayer.learn_qtable(p2_board_backup, p2_action_backup, env, -1)

        # p2 í–‰ë™ ì„ íƒ
        player = -1
        pos = p2_Qplayer.policy(env)
        p2_board_backup = tuple(env.board_a)
        p2_action_backup = pos
        env.board_a[pos] = player
        env.end_check(player)

        # ê²Œì„ì´ ì¢…ë£Œ ìƒíƒœë¼ë©´ ê° í”Œë ˆì´ì–´ì˜ Q-table í•™ìŠµ
        if env.done:
            # ë¹„ê²¼ìœ¼ë©´ ë³´ìƒ 0
            if env.reward == 0:
                p1_Qplayer.learn_qtable(p1_board_backup, p1_action_backup, env, 0)
                p2_Qplayer.learn_qtable(p2_board_backup, p2_action_backup, env, 0)
                draw_score += 1
                break
            else:
                p1_Qplayer.learn_qtable(
                    p1_board_backup, p1_action_backup, env, -1
                )  # p1 íŒ¨ë°°
                p2_Qplayer.learn_qtable(
                    p2_board_backup, p2_action_backup, env, 1
                )  # p2 ìŠ¹ë¦¬
                p2_score += 1
                break

        # ê²Œì„ì´ ëë‚˜ì§€ ì•Šì•˜ë‹¤ë©´ p1ì˜ Q-tableì„ í•™ìŠµ
        p1_Qplayer.learn_qtable(p1_board_backup, p1_action_backup, env, -1)

    # 1000ê²Œì„ë§ˆë‹¤ ê²°ê³¼ í‘œì‹œ
    if j % 1000 == 0:
        print(
            "j = {}, p1 = {}, p2 = {}, draw = {}".format(
                j, p1_score, p2_score, draw_score
            )
        )

print("p1 = {} p2 = {} draw = {}".format(p1_score, p2_score, draw_score))
print("end train")
