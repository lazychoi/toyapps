# í•¨ìˆ˜ ê·¼ì‚¬ : Q-learning

import numpy as np
from tqdm import tqdm
from r01_env_agent import Environment, Agent
from r02_show import show_q_table, show_policy

np.random.seed(0)
env = Environment()
agent = Agent()
gamma = 0.9

# ì´ˆê¸°í™”
# ğ‘£(ğ‘ â”‚ğ’˜) â† ë¯¸ë¶„ ê°€ëŠ¥í•œ í•¨ìˆ˜
# w â† í•¨ìˆ˜ì˜ ê°€ì¤‘ì¹˜ë¥¼ ì„ì˜ì˜ ê°’ìœ¼ë¡œ ì´ˆê¸°í™”
# w[0]+ w[1] * x1  + w[1] * x2
w = np.random.rand(len(agent.action), env.reward.shape[0])
# w -= 0.5    # ì™œ ë¹¼ì§€???

FA_Q_table = np.zeros((env.reward.shape[0], env.reward.shape[1], len(agent.action)))

# í•¨ìˆ˜ë¥¼ í…Œì´ë¸”ì— ì €ì¥
for i in range(env.reward.shape[0]):
    for j in range(env.reward.shape[1]):
        for k in range(len(agent.action)):
            FA_Q_table[i, j, k] = w[k, 0] + w[k, 1] * i + w[k, 2] * j

# í•™ìŠµëœ ì •ì±…ì—ì„œ ìµœì  í–‰ë™ ì¶”ì¶œ
optimal_policy = np.zeros((env.reward.shape[0], env.reward.shape[1]))
for i in range(env.reward.shape[0]):
    for j in range(env.reward.shape[1]):
        optimal_policy[i, j] = np.argmax(FA_Q_table[i, j, :])

print("Before : Function Approximation Q-learning : Q(s,a|w)")
show_q_table(np.round(FA_Q_table, 2), env)
print()
print("Before : Function Approximation Q-learning :optimal policy")
show_policy(optimal_policy, env)
print()
print("Initial w")
print("w = {}".format(np.round(w, 2)))
print()

max_episode = 100000
max_step = 100
alpha = 0.01

print("start Function Approximation : Q-learning")

# ê° ì—í”¼ì†Œë“œ ë°˜ë³µ
for epi in tqdm(range(max_episode)):
    # s ì´ˆê¸°í™”
    i = 0
    j = 0
    agent.set_pos([i, j])

    # ì—í”¼ì†Œë“œì˜ ê° ìŠ¤í… ë°˜ë³µ
    for k in range(max_step):
        pos = agent.get_pos()

        # sì—ì„œ Behavior policyë¡œ í–‰ë™ aë¥¼ ì„ íƒ (Gibbs ì†Œí”„íŠ¸ë§¥ìŠ¤ í•¨ìˆ˜ì‚¬ìš©)
        action = np.zeros(4)
        for act in range(len(agent.action)):
            action[act] = w[act, 0] + w[act, 1] * pos[0] + w[act, 2] * pos[1]
        pr = np.zeros(4)
        for i in range(len(agent.action)):
            pr[i] = np.exp(action[i]) / np.sum(np.exp(action[:]))
        action = np.random.choice(range(0, len(agent.action)), p=pr)

        # í–‰ë™ aë¥¼ ì·¨í•œ í›„ ë³´ìƒ rê³¼ ë‹¤ìŒ ìƒíƒœ s' ê´€ì¸¡
        observation, reward, done = env.move(agent, action)

        # s'ì—ì„œ Target policy í–‰ë™ a'ë¥¼ ì„ íƒ (ğ‘”ğ‘Ÿğ‘’ğ‘’ğ‘‘ğ‘¦)
        next_act = np.zeros(4)
        for act in range(len(agent.action)):
            next_act[act] = np.dot(w[act, 1:], observation) + w[act, 0]
        best_action = np.argmax(next_act)
        now_q = np.dot(w[action, 1:], pos) + w[action, 0]
        next_q = np.dot(w[best_action, 1:], observation) + w[best_action, 0]

        # w â† ğ‘¤ + ğ›¼[ğ‘Ÿ + ğ›¾maxQ'(ğ‘ ',a'|ğ’˜) - Q(ğ‘ ,a|ğ’˜)](ğœ•Q(s,a|ğ’˜))/ğœ•ğ‘¤
        w[action, 0] += alpha * (reward + gamma * next_q - now_q)
        w[action, 1] += alpha * (reward + gamma * next_q - now_q) * pos[0]
        w[action, 2] += alpha * (reward + gamma * next_q - now_q) * pos[1]

        # sê°€ ë§ˆì§€ë§‰ ìƒíƒœë¼ë©´ ì¢…ë£Œ
        if done:
            break

# í•¨ìˆ˜ë¥¼ í…Œì´ë¸”ì— ì €ì¥
for i in range(env.reward.shape[0]):
    for j in range(env.reward.shape[1]):
        for k in range(len(agent.action)):
            FA_Q_table[i, j, k] = w[k, 0] + w[k, 1] * i + w[k, 2] * j

# í•™ìŠµëœ ì •ì±…ì—ì„œ ìµœì  í–‰ë™ ì¶”ì¶œ
for i in range(env.reward.shape[0]):
    for j in range(env.reward.shape[1]):
        optimal_policy[i, j] = np.argmax(FA_Q_table[i, j, :])


print("After : Function Approximation Q-learning : Q(s,a|w)")
show_q_table(np.round(FA_Q_table, 2), env)
print()
print("After : Function Approximation Q-learning :optimal policy")
show_policy(optimal_policy, env)
print()
print("Final w")
print("w = {}".format(np.round(w, 2)))
print()
