# TD Prediction

import numpy as np
from tqdm import tqdm

from r01_env_agent import Environment, Agent
from r02_show import show_v_table

# TD(0) Prediction
np.random.seed(0)

# ì´ˆê¸°í™”
env = Environment()
agent = Agent()
gamma = 0.9

# Ï€â† í‰ê°€í•  ì •ì±…
# ê°€ëŠ¥í•œ ëª¨ë“  í–‰ë™ì´ ë¬´ì‘ìœ„ë¡œ ì„ íƒë˜ë„ë¡ ì§€ì •
# V <- ì„ì˜ì˜ ìƒíƒœ ê°€ì¹˜ í•¨ìˆ˜
V = np.zeros((env.reward.shape[0], env.reward.shape[1]))

# ìµœëŒ€ ì—í”¼ì†Œë“œ ìˆ˜ì™€ ê° ì—í”¼ì†Œë“œì˜ ìµœëŒ€ ê¸¸ì´ ì§€ì •
max_episode = 10000
max_step = 100

alpha = 0.01
print("start TD(0) prediction")

# ê° ì—í”¼ì†Œë“œì— ëŒ€í•´ ë°˜ë³µ
for epi in tqdm(range(max_episode)):
    delta = 0
    i = 0
    j = 0
    agent.set_pos([i, j])

    # ì—í”¼ì†Œë“œì˜ ê° ìŠ¤í… ë°˜ë³µ
    for k in range(max_step):
        pos = agent.get_pos()
        # a <- ìƒíƒœ sì—ì„œ ì •ì±… Ï€ì— ì˜í•´ ê²°ì •ëœ í–‰ë™
        # ê°€ëŠ¥í•œ ëª¨ë“  í–‰ë™ì´ ë¬´ì‘ìœ„ë¡œ ì„ íƒë˜ê²Œ í•¨
        action = np.random.randint(0, 4)

        # í–‰ë™ aë¥¼ ì·¨í•œ í›„ ë³´ìƒ rê³¼ ë‹¤ìŒ ìƒíƒœ s'ì„ ê´€ì¸¡
        # s <- s'
        observation, reward, done = env.move(agent, action)

        # V(ğ‘ )â†V(ğ‘ )+ Î±[ğ‘Ÿ+ğ›¾ğ‘‰(ğ‘ ^)âˆ’ğ‘‰(ğ‘ )]
        V[pos[0], pos[1]] += alpha * (
            reward + gamma * V[observation[0], observation[1]] - V[pos[0], pos[1]]
        )

        # sê°€ ë§ˆì§€ë§‰ ìƒíƒœë©´ ì¢…ë£Œ
        if done:
            break

print("V(s)")
show_v_table(np.round(V, 2), env)
