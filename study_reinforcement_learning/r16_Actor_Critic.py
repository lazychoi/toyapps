# Actor-Critic Algorithm

import numpy as np
from tqdm import tqdm
from r01_env_agent import Environment, Agent
from r02_show import show_v_table, show_q_table, show_policy

np.random.seed(0)
env = Environment()
agent = Agent()
gamma = 0.9

# p(ğ‘ ,ğ‘), ğ‘‰(ğ‘ )â†ì„ì˜ì˜ ê°’
V = np.random.rand(env.reward.shape[0], env.reward.shape[1])
policy = np.random.rand(env.reward.shape[0], env.reward.shape[1], len(agent.action))

# í™•ë¥ ì˜ í•©ì´ 1ì´ ë˜ë„ë¡ ë³€í™˜ -> ì •ê·œí™”(ê°œë³„ê°’/í•©ê³„)
for i in range(env.reward.shape[0]):
    for j in range(env.reward.shape[1]):
        policy[i, j, :] = policy[i, j, :] / np.sum(policy[i, j, :])

max_episode = 10000
max_step = 100

print("start Actor-Critic")
alpha = 0.1

# ê° ì—í”¼ì†Œë“œì— ëŒ€í•´ ë°˜ë³µ :
for epi in tqdm(range(max_episode)):
    # S ì´ˆê¸°í™”
    i = 0
    j = 0
    agent.set_pos([i, j])

    # ì—í”¼ì†Œë“œì˜ ê° ìŠ¤í… ë°˜ë³µ :
    for k in range(max_step):
        # Actor : p(ğ‘ , ğ‘)ë¡œë¶€í„° aë¥¼ ì„ íƒ ( ì˜ˆ: Gibbs softmax method)
        # Gibbs softmax method ë¡œ ì„ íƒë  í™•ë¥ ì„ ì¡°ì •
        # np.exp() -> ê° ìš”ì†Œì— ìì—°ìƒìˆ˜ë¥¼ ë°‘ìœ¼ë¡œ í•˜ëŠ” ì§€ìˆ˜í•¨ìˆ˜ë¥¼ ì ìš©í•˜ì—¬ ë°˜í™˜. np.exp(2) -> e^2
        # í™•ë¥ ì˜ í•©ì„ 1ë¡œ ë§Œë“¤ê¸° ìœ„í•´ ì •ê·œí™”(í•©ê³„ë¡œ ê°œë³„ê°’ì„ ë‚˜ëˆ”)
        pos = agent.get_pos()  # ì—ì´ì „íŠ¸ì˜ í˜„ì¬ ìœ„ì¹˜ ê°€ì ¸ì˜´
        pr = np.zeros(4)
        for i in range(len(agent.action)):  # ê¹ìŠ¤ì˜ ì†Œí”„íŠ¸ë©•ìŠ¤ í•¨ìˆ˜ë¥¼ ì ìš©í•´ ë„¤ ë°©í–¥ì˜ í™•ë¥  ìƒì„±
            pr[i] = np.exp(policy[pos[0], pos[1], i]) / np.sum(
                np.exp(policy[pos[0], pos[1], :])
            )
        action = np.random.choice(range(0, len(agent.action)), p=pr)  # í–‰ë™ ì„ íƒ

        # í–‰ë™ a ì·¨í•œ í›„ ë³´ìƒ r, ë‹¤ìŒ ìƒíƒœ s' ê´€ì¸¡
        observation, reward, done = env.move(agent, action)

        # í¬ë¦¬í‹± í•™ìŠµ
        # Î´t = r(t+1) + Î³V(S(t+1)) - V(St)
        td_error = (
            reward + gamma * V[observation[0], observation[1]] - V[pos[0], pos[1]]
        )

        # ìƒíƒœê°€ì¹˜ í•™ìŠµ(ì—…ë°ì´íŠ¸)
        V[pos[0], pos[1]] += alpha * td_error  # ì‹œí–‰ì°©ì˜¤ * í•™ìŠµë¥ 

        # ì•¡í„° í•™ìŠµ (í–‰ë™ì •ì±… ì—…ë°ì´íŠ¸)
        # p(st,at) = p(st,at) - Î²Î´_t
        policy[pos[0], pos[1], action] += td_error * 0.01  # ì‹œí–‰ì°©ì˜¤ì˜ 1% ë°˜ì˜

        # í™•ë¥ ì— ìŒìˆ˜ê°€ ìˆì„ ê²½ìš° ì–‘ìˆ˜ê°€ ë˜ë„ë¡ ë³´ì •(í¬ë¦¬í‹± í•™ìŠµ ìˆ˜ì‹ì— ëº„ì…ˆì´ ìˆì–´ ìŒìˆ˜ê°€ ë‚˜ì˜¬ ìˆ˜ ìˆìŒ)
        # a = [0.2, 0.4, 0.7, -0.5]
        # a = a - np.min(a)         => [0.7 0.9 1.2 0. ]
        # ê° ìš”ì†Œì— ìŒìˆ˜ ìš”ì†Œë¥¼ ë¹¼ë©´ => ìŒìˆ˜ ìš”ì†Œì˜ ì ˆëŒ€ê°’ì„ ë”í•˜ëŠ” íš¨ê³¼ => ìŒìˆ˜ ìš”ì†ŒëŠ” 0ì´ ë¨
        if np.min(policy[pos[0], pos[1], :]) < 0:
            policy[pos[0], pos[1], :] -= np.min(policy[pos[0], pos[1], :])
        # í™•ë¥ ì˜ í•©ì´ 1ì´ ë˜ë„ë¡ ì •ê·œí™”
        for i in range(env.reward.shape[0]):
            for j in range(env.reward.shape[1]):
                policy[i, j, :] = policy[i, j, :] / np.sum(policy[i, j, :])

        # sê°€ ë§ˆì§€ë§‰ ìƒíƒœë¼ë©´ ì¢…ë£Œ
        if done:
            break

# í•™ìŠµëœ ì •ì±… ì¤‘ ìµœì  í–‰ë™ì„ optimal_policyì— ì €ì¥
optimal_policy = np.zeros((env.reward.shape[0], env.reward.shape[1]))
for i in range(env.reward.shape[0]):
    for j in range(env.reward.shape[1]):
        optimal_policy[i, j] = np.argmax(policy[i, j, :])

print("Actor-Critic : V(s)")
show_v_table(np.round(V, 2), env)
print("Actor-Critic : policy(s,a)")
show_q_table(np.round(policy, 2), env)
print("Actor-Critic : optimal policy")
show_policy(np.round(optimal_policy, 2), env)
